{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio - Aula 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"https://s3.amazonaws.com/automl-example/produtos.csv\",\n",
    "                 delimiter=\";\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) utilizando o df acima carregado, faça:\n",
    "\n",
    "   ### -> Elimine linhas com valores nulos\n",
    "   ### -> Adicione uma nova coluna chamada texto, formada pela composição das colunas nome e descrição\n",
    "   ### -> Quantos Unigramas existem antes e depois de remover stopwords\n",
    "   ### -> Quantos Bigramas existem antes e depois de remover stopwords\n",
    "   ### -> Quantos Trigramas existem antes e depois de remover stopwords\n",
    "   ### -> Quantos verbos e Adverbios existem na nova coluna (utilize: from collections import Counter)\n",
    "   ### -> Quantos unigramas existem após aplicar Stemmer (utilize rslp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nome</th>\n",
       "      <th>descricao</th>\n",
       "      <th>categoria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O Hobbit - 7ª Ed. 2013</td>\n",
       "      <td>Produto NovoBilbo Bolseiro é um hobbit que lev...</td>\n",
       "      <td>livro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Livro - It A Coisa - Stephen King</td>\n",
       "      <td>Produto NovoDurante as férias escolares de 195...</td>\n",
       "      <td>livro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Box  As Crônicas De Gelo E Fogo  Pocket  5 Li...</td>\n",
       "      <td>Produto NovoTodo o reino de Westeros ao alcanc...</td>\n",
       "      <td>livro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Box Harry Potter</td>\n",
       "      <td>Produto Novo e Físico  A série Harry Potter ch...</td>\n",
       "      <td>livro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Livro Origem - Dan Brown</td>\n",
       "      <td>Produto NovoDe Onde Viemos? Para Onde Vamos? R...</td>\n",
       "      <td>livro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                nome  \\\n",
       "0                            O Hobbit - 7ª Ed. 2013    \n",
       "1                 Livro - It A Coisa - Stephen King    \n",
       "2   Box  As Crônicas De Gelo E Fogo  Pocket  5 Li...   \n",
       "3                                  Box Harry Potter    \n",
       "4                          Livro Origem - Dan Brown    \n",
       "\n",
       "                                           descricao categoria  \n",
       "0  Produto NovoBilbo Bolseiro é um hobbit que lev...     livro  \n",
       "1  Produto NovoDurante as férias escolares de 195...     livro  \n",
       "2  Produto NovoTodo o reino de Westeros ao alcanc...     livro  \n",
       "3  Produto Novo e Físico  A série Harry Potter ch...     livro  \n",
       "4  Produto NovoDe Onde Viemos? Para Onde Vamos? R...     livro  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 :\n",
    "df = df[df['descricao'].isna() == False]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2:\n",
    "df['texto'] = df['nome'] + df['descricao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\logonrmlocal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Antes de remover as stopwords: 35466 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<2916x35310 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 269718 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.corpus.stopwords.words('portuguese')\n",
    "stops = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "# Unigramas Antes de remover as stopwords\n",
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df.texto)\n",
    "text_vect = vect.transform(df.texto)\n",
    "print(\"Unigramas Antes de remover as stopwords: %s \" % text_vect.shape[1])\n",
    "\n",
    "# Unigramas Depois de remover as stopwords\n",
    "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
    "vect.fit(df.texto)\n",
    "text_vect = vect.transform(df.texto)\n",
    "print(\"Unigramas depois de remover as stopwords: %s\" %  text_vect.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigramas Antes de remover as stopwords: 159553 \n",
      "Bigramas depois de remover as stopwords: 145409\n"
     ]
    }
   ],
   "source": [
    "# 4:\n",
    "# Bigramas Antes de remover as stopwords\n",
    "vect = CountVectorizer(ngram_range=(2,2))\n",
    "vect.fit(df.texto)\n",
    "text_vect = vect.transform(df.texto)\n",
    "print(\"Bigramas Antes de remover as stopwords: %s \" % text_vect.shape[1])\n",
    "\n",
    "# Bigramas Depois de remover as stopwords\n",
    "vect = CountVectorizer(ngram_range=(2,2), stop_words=stops)\n",
    "vect.fit(df.texto)\n",
    "text_vect = vect.transform(df.texto)\n",
    "print(\"Bigramas depois de remover as stopwords: %s\" %  text_vect.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigramas Antes de remover as stopwords: 228162 \n",
      "Trigramas depois de remover as stopwords: 177869\n"
     ]
    }
   ],
   "source": [
    "# Trigramas Antes de remover as stopwords\n",
    "vect = CountVectorizer(ngram_range=(3,3))\n",
    "vect.fit(df.texto)\n",
    "text_vect = vect.transform(df.texto)\n",
    "print(\"Trigramas Antes de remover as stopwords: %s \" % text_vect.shape[1])\n",
    "\n",
    "# Trigramas Depois de remover as stopwords\n",
    "vect = CountVectorizer(ngram_range=(3,3), stop_words=stops)\n",
    "vect.fit(df.texto)\n",
    "text_vect = vect.transform(df.texto)\n",
    "print(\"Trigramas depois de remover as stopwords: %s\" %  text_vect.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package floresta to\n",
      "[nltk_data]     C:\\Users\\logonrmlocal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package floresta is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('para', 'NN'), ('para', 'NN')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5:\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('floresta')\n",
    "from nltk.corpus import floresta\n",
    "\n",
    "text = nltk.word_tokenize(\"para para\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) crie um tagger baseado em expressões regulares:\n",
    "   ### -> crie uma variável que amazenará uma tupla (expressão, tag)\n",
    "   ### -> o tagger deverá capturar gerúndio (ando, endo, indo), plurais e números cardinais\n",
    "   ### -> utilize nltk.RegexpTagger(variável) para carregar seu tagger\n",
    "   ### -> apresente uma frase teste para cada tipo de expressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
