{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio - Aula 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"https://s3.amazonaws.com/automl-example/produtos.csv\", delimiter=\";\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) utilizando o df acima carregado, faça:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### -> Elimine linhas com valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['descricao'].isna() == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> Adicione uma nova coluna chamada texto, formada pela composição das colunas nome e descrição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['texto'] = df['nome'] + df['descricao']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### -> Quantos Unigramas existem antes e depois de remover stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Unigramas Antes de remover as stopwords: 35466 \n",
      "Unigramas depois de remover as stopwords: 35310\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.corpus.stopwords.words('portuguese')\n",
    "stops = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "# Unigramas Antes de remover as stopwords\n",
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df.texto)\n",
    "text_vect = vect.transform(df.texto)\n",
    "print(\"=========================================================\")\n",
    "print(\"Unigramas Antes de remover as stopwords: %s \" % text_vect.shape[1])\n",
    "\n",
    "# Unigramas Depois de remover as stopwords\n",
    "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
    "vect.fit(df.texto)\n",
    "text_vect = vect.transform(df.texto)\n",
    "print(\"Unigramas depois de remover as stopwords: %s\" %  text_vect.shape[1])\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### -> Quantos Bigramas existem antes e depois de remover stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Bigramas Antes de remover as stopwords: 159553 \n",
      "Bigramas depois de remover as stopwords: 145409\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# Bigramas Antes de remover as stopwords\n",
    "vect = CountVectorizer(ngram_range=(2,2))\n",
    "vect.fit(df.texto)\n",
    "text_vect = vect.transform(df.texto)\n",
    "print(\"=========================================================\")\n",
    "print(\"Bigramas Antes de remover as stopwords: %s \" % text_vect.shape[1])\n",
    "\n",
    "# Bigramas Depois de remover as stopwords\n",
    "vect = CountVectorizer(ngram_range=(2,2), stop_words=stops)\n",
    "vect.fit(df.texto)\n",
    "text_vect = vect.transform(df.texto)\n",
    "print(\"Bigramas depois de remover as stopwords: %s\" %  text_vect.shape[1])\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### -> Quantos Trigramas existem antes e depois de remover stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigramas Antes de remover as stopwords: 228162 \n",
      "Trigramas depois de remover as stopwords: 177869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<2916x177869 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 346350 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trigramas Antes de remover as stopwords\n",
    "vect = CountVectorizer(ngram_range=(3,3))\n",
    "vect.fit(df.texto)\n",
    "text_vect = vect.transform(df.texto)\n",
    "print(\"=========================================================\")\n",
    "print(\"Trigramas Antes de remover as stopwords: %s \" % text_vect.shape[1])\n",
    "\n",
    "# Trigramas Depois de remover as stopwords\n",
    "vect = CountVectorizer(ngram_range=(3,3), stop_words=stops)\n",
    "vect.fit(df.texto)\n",
    "text_vect = vect.transform(df.texto)\n",
    "print(\"Trigramas depois de remover as stopwords: %s\" %  text_vect.shape[1])\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### -> Quantos verbos e Adverbios existem na nova coluna (utilize: from collections import Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package floresta to\n",
      "[nltk_data]     C:\\Users\\Andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package floresta is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "nltk.download('floresta')\n",
    "from nltk.corpus import floresta\n",
    "\n",
    "v = \"\".join([i for i in df['texto']])\n",
    "text = nltk.word_tokenize(v)\n",
    "value_tags = nltk.pos_tag(text)\n",
    "tags = [i[1][0:2] for i in value_tags]\n",
    "counter = Counter(tags)\n",
    "print(\"=========================================================\")\n",
    "print(\"Quantidade de verbos: %d\" % counter['VB'])\n",
    "print(\"Quantidade de adverbios: %d\" % counter['RB'])\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### -> Quantos unigramas existem após aplicar Stemmer (utilize rslp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\Andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "=========================================================\n",
      "Unigramas antes de aplicar stemmer: 35466\n",
      "Unigramas apos aplicar stemmer: 24385 \n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('rslp')\n",
    "from nltk.stem.rslp import RSLPStemmer\n",
    "rslp = RSLPStemmer()\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df['texto'])\n",
    "text_vect = vect.transform(df['texto'])\n",
    "\n",
    "stem_words = [rslp.stem(word) for word in vect.get_feature_names()]\n",
    "counter = Counter(stem_words)\n",
    "print(\"=========================================================\")\n",
    "print(\"Unigramas antes de aplicar stemmer: %d\" % text_vect.shape[1])\n",
    "print(\"Unigramas apos aplicar stemmer: %d \" % len(counter))\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) crie um tagger baseado em expressões regulares:\n",
    "   ### -> crie uma variável que amazenará uma tupla (expressão, tag)\n",
    "   ### -> o tagger deverá capturar gerúndio (ando, endo, indo), plurais e números cardinais\n",
    "   ### -> utilize nltk.RegexpTagger(variável) para carregar seu tagger\n",
    "   ### -> apresente uma frase teste para cada tipo de expressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
