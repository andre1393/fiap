{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio - Aula 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"https://s3.amazonaws.com/automl-example/produtos.csv\", delimiter=\";\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) utilizando o df acima carregado, faça:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### -> Elimine linhas com valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2916"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['descricao'].isna() == False]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> Adicione uma nova coluna chamada texto, formada pela composição das colunas nome e descrição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['texto'] = df['nome'] + df['descricao']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### -> Quantos Unigramas existem antes e depois de remover stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\logonrmlocal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "=========================================================\n",
      "Unigramas Antes de remover as stopwords: 35466 \n",
      "Unigramas depois de remover as stopwords: 35310\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.corpus.stopwords.words('portuguese')\n",
    "stops = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "# Unigramas Antes de remover as stopwords\n",
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df.texto)\n",
    "text_vect = vect.transform(df.texto)\n",
    "print(\"=========================================================\")\n",
    "print(\"Unigramas Antes de remover as stopwords: %s \" % text_vect.shape[1])\n",
    "\n",
    "# Unigramas Depois de remover as stopwords\n",
    "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
    "vect.fit(df.texto)\n",
    "text_vect = vect.transform(df.texto)\n",
    "print(\"Unigramas depois de remover as stopwords: %s\" %  text_vect.shape[1])\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### -> Quantos Bigramas existem antes e depois de remover stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Bigramas Antes de remover as stopwords: 159553 \n",
      "Bigramas depois de remover as stopwords: 145409\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# Bigramas Antes de remover as stopwords\n",
    "vect = CountVectorizer(ngram_range=(2,2))\n",
    "vect.fit(df.texto)\n",
    "text_vect = vect.transform(df.texto)\n",
    "print(\"=========================================================\")\n",
    "print(\"Bigramas Antes de remover as stopwords: %s \" % text_vect.shape[1])\n",
    "\n",
    "# Bigramas Depois de remover as stopwords\n",
    "vect = CountVectorizer(ngram_range=(2,2), stop_words=stops)\n",
    "vect.fit(df.texto)\n",
    "text_vect = vect.transform(df.texto)\n",
    "print(\"Bigramas depois de remover as stopwords: %s\" %  text_vect.shape[1])\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### -> Quantos Trigramas existem antes e depois de remover stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Trigramas Antes de remover as stopwords: 228162 \n",
      "Trigramas depois de remover as stopwords: 177869\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# Trigramas Antes de remover as stopwords\n",
    "vect = CountVectorizer(ngram_range=(3,3))\n",
    "vect.fit(df.texto)\n",
    "text_vect = vect.transform(df.texto)\n",
    "print(\"=========================================================\")\n",
    "print(\"Trigramas Antes de remover as stopwords: %s \" % text_vect.shape[1])\n",
    "\n",
    "# Trigramas Depois de remover as stopwords\n",
    "vect = CountVectorizer(ngram_range=(3,3), stop_words=stops)\n",
    "vect.fit(df.texto)\n",
    "text_vect = vect.transform(df.texto)\n",
    "print(\"Trigramas depois de remover as stopwords: %s\" %  text_vect.shape[1])\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### -> Quantos verbos e Adverbios existem na nova coluna (utilize: from collections import Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\logonrmlocal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\logonrmlocal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package floresta to\n",
      "[nltk_data]     C:\\Users\\logonrmlocal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\floresta.zip.\n",
      "=========================================================\n",
      "Quantidade de verbos: 40724\n",
      "Quantidade de adverbios: 3644\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "nltk.download('floresta')\n",
    "from nltk.corpus import floresta\n",
    "\n",
    "v = \"\".join([i for i in df['texto']])\n",
    "text = nltk.word_tokenize(v)\n",
    "value_tags = nltk.pos_tag(text)\n",
    "tags = [i[1][0:2] for i in value_tags]\n",
    "counter = Counter(tags)\n",
    "print(\"=========================================================\")\n",
    "print(\"Quantidade de verbos: %d\" % counter['VB'])\n",
    "print(\"Quantidade de adverbios: %d\" % counter['RB'])\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### -> Quantos unigramas existem após aplicar Stemmer (utilize rslp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\logonrmlocal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping stemmers\\rslp.zip.\n",
      "=========================================================\n",
      "Unigramas antes de aplicar stemmer: 35466\n",
      "Unigramas apos aplicar stemmer: 24385 \n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('rslp')\n",
    "from nltk.stem.rslp import RSLPStemmer\n",
    "rslp = RSLPStemmer()\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df['texto'])\n",
    "text_vect = vect.transform(df['texto'])\n",
    "\n",
    "stem_words = [rslp.stem(word) for word in vect.get_feature_names()]\n",
    "counter = Counter(stem_words)\n",
    "print(\"=========================================================\")\n",
    "print(\"Unigramas antes de aplicar stemmer: %d\" % text_vect.shape[1])\n",
    "print(\"Unigramas apos aplicar stemmer: %d \" % len(counter))\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) crie um tagger baseado em expressões regulares:\n",
    "   ### -> crie uma variável que amazenará uma tupla (expressão, tag)\n",
    "   ### -> o tagger deverá capturar gerúndio (ando, endo, indo), plurais e números cardinais\n",
    "   ### -> utilize nltk.RegexpTagger(variável) para carregar seu tagger\n",
    "   ### -> apresente uma frase teste para cada tipo de expressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('eu', None), ('estou', None), ('estudando', 'VBG'), ('nlp', None)]\n",
      "[('Eu', None), ('tenho', None), ('10', 'CD'), ('dez', 'CD'), ('aulas', 'NNS'), ('de', None), ('sabado', None), ('no', None), ('curso', None)]\n",
      "[('eu', None), ('nasci', None), ('em', None), ('1993', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "pattern = [(r'\\w*ando\\b|\\w*endo\\b|\\w*indo\\b', 'VBG'), (r'[U u]m|[D d]ois|[T t]res|[Q q]uatro|[C c]inco|[S s]eis|[S s]ete|[O o]ito|[N n]ove|[D d]ez', 'CD'), (r'\\w*s\\b', 'NNS')]\n",
    "tagger = nltk.RegexpTagger(pattern)\n",
    "print(tagger.tag(nltk.word_tokenize(\"eu estou estudando nlp\")))\n",
    "print(tagger.tag(nltk.word_tokenize(\"Eu tenho  dez aulas de sabado no curso\")))\n",
    "print(tagger.tag(nltk.word_tokenize(\"o MBA na fiap dura um ano\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
